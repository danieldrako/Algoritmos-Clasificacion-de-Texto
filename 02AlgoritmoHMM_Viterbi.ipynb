{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO//RkVPmsDTof4RvJ6l4an",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danieldrako/Algoritmos-Clasificacion-de-Texto/blob/main/02AlgoritmoHMM_Viterbi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z9dFb8kqC_cO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc0d048-ef9a-487b-b30d-a9dd8a19cfc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting conllu\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.5.2\n",
            "Cloning into 'UD_Spanish-AnCora'...\n",
            "remote: Enumerating objects: 928, done.\u001b[K\n",
            "remote: Counting objects: 100% (181/181), done.\u001b[K\n",
            "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
            "remote: Total 928 (delta 128), reused 174 (delta 121), pack-reused 747\u001b[K\n",
            "Receiving objects: 100% (928/928), 337.55 MiB | 35.60 MiB/s, done.\n",
            "Resolving deltas: 100% (653/653), done.\n"
          ]
        }
      ],
      "source": [
        "# instalacion de dependencias previas\n",
        "!pip install conllu\n",
        "!git clone https://github.com/UniversalDependencies/UD_Spanish-AnCora.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAHDLyQQcboh"
      },
      "source": [
        "# Carga del modelo HMM previamente entrenado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jwZedC6RVJy"
      },
      "source": [
        "# cargamos las probabilidades del modelo HMM\n",
        "import numpy as np\n",
        "transitionProbdict = np.load('transitionHMM.npy', allow_pickle='TRUE').item()\n",
        "emissionProbdict = np.load('emissionHMM.npy', allow_pickle='TRUE').item()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezi9gcquUw55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e388a53c-1655-4ce8-cf1f-db89be870e82"
      },
      "source": [
        "# identificamos las categorias gramaticales 'upos' unicas en el corpus\n",
        "stateSet = set([w.split('|')[1] for w in list(emissionProbdict.keys())])\n",
        "stateSet"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ADJ',\n",
              " 'ADP',\n",
              " 'ADV',\n",
              " 'AUX',\n",
              " 'CCONJ',\n",
              " 'DET',\n",
              " 'INTJ',\n",
              " 'NOUN',\n",
              " 'NUM',\n",
              " 'PART',\n",
              " 'PRON',\n",
              " 'PROPN',\n",
              " 'PUNCT',\n",
              " 'SCONJ',\n",
              " 'SYM',\n",
              " 'VERB',\n",
              " '_'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aVD0jboWKGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7748fb8-e5c9-4bea-a83d-d8d818ce6df4"
      },
      "source": [
        "# enumeramos las categorias con numeros para asignar a \n",
        "# las columnas de la matriz de Viterbi\n",
        "tagStateDict = {}\n",
        "for i, state in enumerate(stateSet):\n",
        "  tagStateDict[state] = i\n",
        "tagStateDict"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'NOUN': 0,\n",
              " 'CCONJ': 1,\n",
              " 'PROPN': 2,\n",
              " 'AUX': 3,\n",
              " 'ADV': 4,\n",
              " 'NUM': 5,\n",
              " '_': 6,\n",
              " 'DET': 7,\n",
              " 'SCONJ': 8,\n",
              " 'VERB': 9,\n",
              " 'ADJ': 10,\n",
              " 'INTJ': 11,\n",
              " 'PART': 12,\n",
              " 'PRON': 13,\n",
              " 'SYM': 14,\n",
              " 'PUNCT': 15,\n",
              " 'ADP': 16}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SNuWx-ScSTg"
      },
      "source": [
        "# Distribucion inicial de estados latentes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "put9Dyk1Yl2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebf5ff0f-2ff3-4e6f-9f40-77457214fe8e"
      },
      "source": [
        "# Calculamos distribución inicial de estados\n",
        "initTagStateProb = {} # \\rho_i^{(0)}\n",
        "from conllu import parse_incr \n",
        "wordList = []\n",
        "data_file = open(\"UD_Spanish-AnCora/es_ancora-ud-dev.conllu\", \"r\", encoding=\"utf-8\")\n",
        "count = 0 # cuenta la longitud del corpus\n",
        "for tokenlist in parse_incr(data_file):\n",
        "  count += 1\n",
        "  tag = tokenlist[0]['upos']\n",
        "  if tag in initTagStateProb.keys():\n",
        "    initTagStateProb[tag] += 1\n",
        "  else:\n",
        "    initTagStateProb[tag] = 1\n",
        "\n",
        "for key in initTagStateProb.keys():\n",
        "  initTagStateProb[key] /= count\n",
        "\n",
        "initTagStateProb"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DET': 0.36275695284159615,\n",
              " 'PROPN': 0.1124546553808948,\n",
              " 'ADP': 0.15538089480048367,\n",
              " 'PRON': 0.06348246674727932,\n",
              " 'SCONJ': 0.02418379685610641,\n",
              " 'ADV': 0.056831922611850064,\n",
              " 'PUNCT': 0.08222490931076179,\n",
              " 'VERB': 0.021160822249093107,\n",
              " 'ADJ': 0.010882708585247884,\n",
              " 'CCONJ': 0.032648125755743655,\n",
              " 'NOUN': 0.02720677146311971,\n",
              " '_': 0.009068923821039904,\n",
              " 'INTJ': 0.0006045949214026602,\n",
              " 'AUX': 0.019347037484885126,\n",
              " 'NUM': 0.01995163240628779,\n",
              " 'PART': 0.0018137847642079807}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5Rltqj6bbcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553a7aea-e29c-4724-b7bf-d5ead89e2659"
      },
      "source": [
        "# verificamos que la suma de las probabilidades es 1 (100%)\n",
        "#np.array([initTagStateProb[k] for k in initTagStateProb.keys()]).sum() \n",
        "#O de otro modo\n",
        "np.array(list(initTagStateProb.values())).sum() "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjnAshwzxrKZ"
      },
      "source": [
        "# Construcción del algoritmo de Viterbi\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX-_MnPexnm0"
      },
      "source": [
        "Dada una secuencia de palabras $\\{p_1, p_2, \\dots, p_n \\}$, y un conjunto de categorias gramaticales dadas por la convención `upos`, se considera la matriz de probabilidades de Viterbi así:\n",
        "\n",
        "$$\n",
        "\\begin{array}{c c}\n",
        "\\begin{array}{c c c c}\n",
        "\\text{ADJ} \\\\\n",
        "\\text{ADV}\\\\\n",
        "\\text{PRON} \\\\\n",
        "\\vdots \\\\\n",
        "{}\n",
        "\\end{array} \n",
        "&\n",
        "\\left[\n",
        "\\begin{array}{c c c c}\n",
        "\\nu_1(\\text{ADJ}) & \\nu_2(\\text{ADJ}) & \\dots  & \\nu_n(\\text{ADJ})\\\\\n",
        "\\nu_1(\\text{ADV}) & \\nu_2(\\text{ADV}) & \\dots  & \\nu_n(\\text{ADV})\\\\ \n",
        "\\nu_1(\\text{PRON}) & \\nu_2(\\text{PRON}) & \\dots  & \\nu_n(\\text{PRON})\\\\\n",
        "\\vdots & \\vdots & \\dots & \\vdots \\\\ \\hdashline\n",
        "p_1 & p_2 & \\dots & p_n \n",
        "\\end{array}\n",
        "\\right] \n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Donde las probabilidades de la primera columna (para una categoria $i$) están dadas por: \n",
        "\n",
        "$$\n",
        "\\nu_1(i) = \\underbrace{\\rho_i^{(0)}}_{\\text{probabilidad inicial}} \\times \\underbrace{P(p_1 \\vert i)}_{\\text{emisión}}\n",
        "$$\n",
        "\n",
        "luego, para la segunda columna (dada una categoria $j$) serán: \n",
        "\n",
        "$$\n",
        "\\nu_2(j) = \\max_i \\{ \\nu_1(i) \\times \\underbrace{P(j \\vert i)}_{\\text{transición}} \\times \\underbrace{P(p_2 \\vert j)}_{\\text{emisión}} \\}\n",
        "$$\n",
        "\n",
        "así, en general las probabilidades para la columna $t$ estarán dadas por: \n",
        "\n",
        "$$\n",
        "\\nu_{t}(j) = \\max_i \\{ \\overbrace{\\nu_{t-1}(i)}^{\\text{estado anterior}} \\times \\underbrace{P(j \\vert i)}_{\\text{transición}} \\times \\underbrace{P(p_t \\vert j)}_{\\text{emisión}} \\}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAyO788xPKra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df4fba6e-293b-4c8d-b0ac-f8f97f287436"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize # importamos el tokenizador"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sJhQ35m5ASB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8db7e84-8bbc-4b6b-ed0d-0a6308d06b30"
      },
      "source": [
        "#@title Función que retorna la matriz de viterbi que corresponde a la secuencia de palbras que pasarán como argumento.\n",
        "# tambien necesita las matrices de probabilidad, el diccionario de categorias y el diccionario que relaciona las categorias con sus filas en la matriz\n",
        "def ViterbiMatrix(secuencia, transitionProbdict=transitionProbdict, emissionProbdict=emissionProbdict, \n",
        "            tagStateDict=tagStateDict, initTagStateProb=initTagStateProb):\n",
        "  seq = word_tokenize(secuencia)#tokenizador\n",
        "  viterbiProb = np.zeros((17, len(seq)))  # upos tiene 17 categorias, matriz llena de ceros\n",
        "\n",
        "  # inicialización primera columna\n",
        "  for key in tagStateDict.keys():\n",
        "    tag_row = tagStateDict[key]\n",
        "    word_tag = seq[0].lower()+'|'+key\n",
        "    if word_tag in emissionProbdict.keys():# si esta pareja etiqueta|palabra está en el diccionario de probabilidades de emisión\n",
        "      viterbiProb[tag_row, 0] = initTagStateProb[key]*emissionProbdict[word_tag] #tokenizador\n",
        "\n",
        "  # computo de las siguientes columnas\n",
        "  for col in range(1, len(seq)):\n",
        "    for key in tagStateDict.keys():\n",
        "      tag_row = tagStateDict[key]\n",
        "      word_tag = seq[col].lower()+'|'+key\n",
        "      if word_tag in emissionProbdict.keys():\n",
        "        # miramos estados de la col anterior\n",
        "        possible_probs = []\n",
        "        for key2 in tagStateDict.keys(): \n",
        "          tag_row2 = tagStateDict[key2]\n",
        "          tag_prevtag = key+'|'+key2\n",
        "          if tag_prevtag in transitionProbdict.keys():\n",
        "            if viterbiProb[tag_row2, col-1]>0:\n",
        "              possible_probs.append(\n",
        "                  viterbiProb[tag_row2, col-1]*transitionProbdict[tag_prevtag]*emissionProbdict[word_tag])\n",
        "        #viterbiProb[tag_row, col] = max(possible_probs)\n",
        "  \n",
        "  return viterbiProb\n",
        "\n",
        "matrix = ViterbiMatrix('La palabra es muy grande')\n",
        "matrix"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.08447641, 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.00057423, 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9CYMtNpuoKq"
      },
      "source": [
        "#@title Función que retorna los tags\n",
        "def ViterbiTags(secuencia, transitionProbdict=transitionProbdict, emissionProbdict=emissionProbdict, \n",
        "            tagStateDict=tagStateDict, initTagStateProb=initTagStateProb):\n",
        "  seq = word_tokenize(secuencia)\n",
        "  viterbiProb = np.zeros((17, len(seq)))  # upos tiene 17 categorias\n",
        "\n",
        "  # inicialización primera columna\n",
        "  for key in tagStateDict.keys():\n",
        "    tag_row = tagStateDict[key]\n",
        "    word_tag = seq[0].lower()+'|'+key\n",
        "    if word_tag in emissionProbdict.keys():\n",
        "      viterbiProb[tag_row, 0] = initTagStateProb[key]*emissionProbdict[word_tag]\n",
        "\n",
        "  # computo de las siguientes columnas\n",
        "  for col in range(1, len(seq)):\n",
        "    for key in tagStateDict.keys():\n",
        "      tag_row = tagStateDict[key]\n",
        "      word_tag = seq[col].lower()+'|'+key\n",
        "      if word_tag in emissionProbdict.keys():\n",
        "        # miramos estados de la col anterior\n",
        "        possible_probs = []\n",
        "        for key2 in tagStateDict.keys(): \n",
        "          tag_row2 = tagStateDict[key2]\n",
        "          tag_prevtag = key+'|'+key2\n",
        "          if tag_prevtag in transitionProbdict.keys():\n",
        "            if viterbiProb[tag_row2, col-1]>0:\n",
        "              possible_probs.append(\n",
        "                  viterbiProb[tag_row2, col-1]*transitionProbdict[tag_prevtag]*emissionProbdict[word_tag])\n",
        "        #viterbiProb[tag_row, col] = max(possible_probs)\n",
        "\n",
        "    # contruccion de secuencia de tags\n",
        "    res = []\n",
        "    for i, p in enumerate(seq):\n",
        "      for tag in tagStateDict.keys():\n",
        "        if tagStateDict[tag] == np.argmax(viterbiProb[:, i]): #cada palabra está asociada a una columna\n",
        "          res.append((p, tag))\n",
        "      \n",
        "  return res\n",
        "\n",
        "matrix = ViterbiMatrix('La palabra es muy grande')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwOk8ABlx13k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e09a3ec-e776-4d43-c171-3b55c8934b84"
      },
      "source": [
        "ViterbiTags('La palabra es muy grande')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('La', 'DET'),\n",
              " ('palabra', 'NOUN'),\n",
              " ('es', 'NOUN'),\n",
              " ('muy', 'NOUN'),\n",
              " ('grande', 'NOUN')]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnLT12Qx5D78"
      },
      "source": [
        "# Entrenamiento directo de HMM con NLTK\n",
        "\n",
        "* clase en python (NLTK) de HMM: https://www.nltk.org/_modules/nltk/tag/hmm.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVyCuawh5Eqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77c038d3-1fdf-480f-a06d-422b51e80633"
      },
      "source": [
        "#@title ejemplo con el Corpus Treebank en ingles\n",
        "import nltk\n",
        "nltk.download('treebank')\n",
        "from nltk.corpus import treebank\n",
        "train_data = treebank.tagged_sents()[:3900]#frases tageadas hasta la sentencia 3900"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_DomEIM5Hif",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d36de3-959d-45fc-9c28-b7acd198c8b8"
      },
      "source": [
        "#@title estructura de la data de entrenamiento\n",
        "train_data"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtknnYIi5KdG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c7b22c-0b70-460a-f47b-9fb61b2427cd"
      },
      "source": [
        "#@title HMM pre-construido en NLTK\n",
        "from nltk.tag import hmm\n",
        "tagger = hmm.HiddenMarkovModelTrainer().train_supervised(train_data) #esta parte hace el entraniento supervisado  del modelo de cadenas de Markov sobre el train_data\n",
        "tagger"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<HiddenMarkovModelTagger 46 states and 12385 output symbols>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLG-QzKc5OM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "938dc17a-93a0-4e1f-b249-84ed14abde43"
      },
      "source": [
        "tagger.tag(\"Dwayne The Rock Johnson will get old\".split())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Dwayne', 'NNP'),\n",
              " ('The', 'NNP'),\n",
              " ('Rock', 'NNP'),\n",
              " ('Johnson', 'NNP'),\n",
              " ('will', 'NNP'),\n",
              " ('get', 'NNP'),\n",
              " ('old', 'NNP')]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.tag(\"David will get old\".split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc-Cu-VShqyM",
        "outputId": "e0a50111-d470-4117-82f9-9a8278f21682"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('David', 'NNP'), ('will', 'MD'), ('get', 'VB'), ('old', 'JJ')]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGLYRUBb5Wni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cf49132-fca4-40c0-b0b4-8ff8c843e414"
      },
      "source": [
        "#@title training accuracy\n",
        "tagger.evaluate(treebank.tagged_sents()[:3900])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9815403947224078"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN-Bgfk-pI0m"
      },
      "source": [
        "## Ejercicio de práctica\n",
        "\n",
        "**Objetivo:** Entrena un HMM usando la clase `hmm.HiddenMarkovModelTrainer()` sobre el dataset `UD_Spanish_AnCora`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrtTL4ihxfiq"
      },
      "source": [
        "1. **Pre-procesamiento:** En el ejemplo anterior usamos el dataset en ingles `treebank`, el cual viene con una estructura diferente a la de `AnCora`, en esta parte escribe código para transformar la estructura de `AnCora` de manera que quede igual al `treebank` que usamos así:\n",
        "\n",
        "$$\\left[ \\left[ (\\text{'El'}, \\text{'DET'}), (\\dots), \\dots\\right], \\left[\\dots \\right] \\right]$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X8qg5Fc5ahS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51844287-13c7-430d-f754-425b3c08212c"
      },
      "source": [
        "# desarrolla tu código aquí \n",
        "!pip install conllu\n",
        "!git clone https://github.com/UniversalDependencies/UD_Spanish-AnCora.git\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.7/dist-packages (4.5.2)\n",
            "fatal: destination path 'UD_Spanish-AnCora' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#leyendo el corpus AnCora\n",
        "from conllu import parse_incr \n",
        "data_file = open(\"UD_Spanish-AnCora/es_ancora-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
        "data_array = []\n",
        "for tokenlist in parse_incr(data_file):\n",
        "  tokenized_text = []\n",
        "  for token in tokenlist:\n",
        "    tokenized_text.append((token['form'], token['upos']))\n",
        "  data_array.append(tokenized_text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sBUvM-o2iv9q"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_array[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHvKvc-5jGNi",
        "outputId": "6d11a781-9286-47d5-a5be-837ab59ae05b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Las', 'DET'),\n",
              "  ('reservas', 'NOUN'),\n",
              "  ('de', 'ADP'),\n",
              "  ('oro', 'NOUN'),\n",
              "  ('y', 'CCONJ'),\n",
              "  ('divisas', 'NOUN'),\n",
              "  ('de', 'ADP'),\n",
              "  ('Rusia', 'PROPN'),\n",
              "  ('subieron', 'VERB'),\n",
              "  ('800', 'NUM'),\n",
              "  ('millones', 'NOUN'),\n",
              "  ('de', 'ADP'),\n",
              "  ('dólares', 'NOUN'),\n",
              "  ('y', 'CCONJ'),\n",
              "  ('el', 'DET'),\n",
              "  ('26', 'NUM'),\n",
              "  ('de', 'ADP'),\n",
              "  ('mayo', 'NOUN'),\n",
              "  ('equivalían', 'VERB'),\n",
              "  ('a', 'ADP'),\n",
              "  ('19.100', 'NUM'),\n",
              "  ('millones', 'NOUN'),\n",
              "  ('de', 'ADP'),\n",
              "  ('dólares', 'NOUN'),\n",
              "  (',', 'PUNCT'),\n",
              "  ('informó', 'VERB'),\n",
              "  ('hoy', 'ADV'),\n",
              "  ('un', 'DET'),\n",
              "  ('comunicado', 'NOUN'),\n",
              "  ('del', '_'),\n",
              "  ('de', 'ADP'),\n",
              "  ('el', 'DET'),\n",
              "  ('Banco', 'PROPN'),\n",
              "  ('Central', 'PROPN'),\n",
              "  ('.', 'PUNCT')],\n",
              " [('Según', 'ADP'),\n",
              "  ('el', 'DET'),\n",
              "  ('informe', 'NOUN'),\n",
              "  (',', 'PUNCT'),\n",
              "  ('el', 'DET'),\n",
              "  ('19', 'NUM'),\n",
              "  ('de', 'ADP'),\n",
              "  ('mayo', 'NOUN'),\n",
              "  ('las', 'DET'),\n",
              "  ('reservas', 'NOUN'),\n",
              "  ('de', 'ADP'),\n",
              "  ('oro', 'NOUN'),\n",
              "  ('y', 'CCONJ'),\n",
              "  ('divisas', 'NOUN'),\n",
              "  ('del', '_'),\n",
              "  ('de', 'ADP'),\n",
              "  ('el', 'DET'),\n",
              "  ('Banco', 'PROPN'),\n",
              "  ('Central', 'PROPN'),\n",
              "  ('eran', 'AUX'),\n",
              "  ('de', 'ADP'),\n",
              "  ('18.300', 'NUM'),\n",
              "  ('millones', 'NOUN'),\n",
              "  ('de', 'ADP'),\n",
              "  ('dólares', 'NOUN'),\n",
              "  ('.', 'PUNCT')]]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c5qZ4rLNjNQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_OYeCVQrZAK"
      },
      "source": [
        "2. **Entrenamiento:** Una vez que el dataset esta con la estructura correcta, utiliza la clase `hmm.HiddenMarkovModelTrainer()` para entrenar con el $80 \\%$ del dataset como conjunto de `entrenamiento` y $20 \\%$ para el conjunto de `test`.\n",
        "\n",
        "**Ayuda:** Para la separacion entre conjuntos de entrenamiento y test, puedes usar la funcion de Scikit Learn: \n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "En este punto el curso de Machine Learning con Scikit Learn es un buen complemento para entender mejor las funcionalidades de Scikit Learn: https://platzi.com/cursos/scikitlearn-ml/ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZpAIB87sTqL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46a2f06e-3390-49a8-a622-684c8df576a7"
      },
      "source": [
        "# desarrolla tu código aquí\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_data, test_data = train_test_split(data_array, test_size=0.2, random_state=42)\n",
        "print(len(train_data))\n",
        "print(len(test_data))\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11429\n",
            "2858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import hmm\n",
        "tagger = hmm.HiddenMarkovModelTrainer().train_supervised(train_data)\n",
        "tagger"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8_pyRlxj8QA",
        "outputId": "3c028c16-f841-431a-d57f-c4f54d26d64c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<HiddenMarkovModelTagger 18 states and 34366 output symbols>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLS54wqsu9OK"
      },
      "source": [
        "3. **Validación del modelo:** Un vez entrenado el `tagger`, calcula el rendimiento del modelo (usando `tagger.evaluate()`) para los conjuntos de `entrenamiento` y `test`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEwZIG8Du98v"
      },
      "source": [
        "#desarrolla tu código aquí\n",
        "tagger = hmm.HiddenMarkovModelTrainer().train_supervised(data_array)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_file = open(\"UD_Spanish-AnCora/es_ancora-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
        "test_array = []\n",
        "for tokenlist in parse_incr(data_file):\n",
        "  tokenized_text = []\n",
        "  for token in tokenlist:\n",
        "    tokenized_text.append((token['form'], token['upos']))\n",
        "  test_array.append(tokenized_text)\n",
        "len(test_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCUr6zLUkHJO",
        "outputId": "0328bb12-44c0-480d-a4ec-5b9e7d5dec4b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14287"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalamos conllu para leer el corpus\n",
        "!pip install conllu\n",
        "from conllu import parse_incr \n",
        "data_file = open(\"UD_Spanish-AnCora/es_ancora-ud-dev.conllu\", \"r\", encoding=\"utf-8\") \n",
        "\n",
        "# Bajamos el corpus de AnCora\n",
        "!git clone https://github.com/UniversalDependencies/UD_Spanish-AnCora.git\n",
        "\n",
        "# Hacemos la transformacion del corpus al formato requerido\n",
        "wordList = []\n",
        "for tokenlist in parse_incr(data_file): \n",
        "  wordList2 = []\n",
        "  for token in tokenlist:\n",
        "    tag = token['upos']\n",
        "    valor = token['form']\n",
        "    wordList2.append((valor,tag)) \n",
        "  wordList.append(wordList2)\n",
        "\n",
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separamos el corpus\n",
        "wordList_train, wordList_test= train_test_split(wordList, test_size=0.20, random_state=42)\n",
        "\n",
        "# Entrenamos el modelo\n",
        "tagger = hmm.HiddenMarkovModelTrainer().train_supervised(wordList_train)\n",
        "tagger\n",
        "\n",
        "print(tagger.evaluate(wordList_test))\n",
        "print(tagger.evaluate(wordList_train))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5LUgZ_1kUvo",
        "outputId": "e42e9c67-d3e7-4003-e742-3f969bbfd471"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.7/dist-packages (4.5.2)\n",
            "fatal: destination path 'UD_Spanish-AnCora' already exists and is not an empty directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2773140078891845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9858684192817568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Se Podria decir que el modelo está sobre ajustado "
      ],
      "metadata": {
        "id": "ctAsqoqxkhGg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}